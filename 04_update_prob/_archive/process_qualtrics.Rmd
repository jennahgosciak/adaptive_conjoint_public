---
title: "Process Qualtrics Data, Treatment Assignment Updating"
output: rmarkdown::github_document
date: "2023-11-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup Data Using Qualtrics API

* Load data directly from Qualtrics
* Can store API Key and credentials in `.renviron`

```{r, load packages, message=F, warning=F}
library(tidyverse)
library(qualtRics)
library(magrittr)
library(RColorBrewer)

knitr::opts_chunk$set(cache.extra = 2023)

source("../plotting_functions.R")
```

```{r, setup qualtrics}
# datacenterid <- ""
# url <- str_glue("https://{datacenterid}.qualtrics.com")
# api_key <- ""
#
# qualtrics_api_credentials(api_key = api_key,
#                           base_url = url,
#                           install = TRUE,
#                           overwrite=T)

readRenviron("~/.Renviron")
# load survey data
surveys <- all_surveys()
# select political candidates survey ID
pc_id <- surveys[surveys["name"] == "Political Candidates", ][["id"]]

pc_survey <- fetch_survey(
  surveyID = pc_id,
  verbose = TRUE
)
```

## Survey Validation

```{r}
pc_survey %>%
  distinct(Status)

# validate survey logic on test data
pc_survey %>%
  filter(Status == "Survey Test")

# should be missing if do not consent
pc_survey %>%
  filter(Status == "Survey Test", Consent == "I do not consent to participate") %>%
  mutate(Q2 = as.character(Q2)) %>%
  distinct(`Q2`) %>%
  is.na() %>%
  stopifnot()

# note: question was different prior to 11/06
# check that if failing prescreen question about location in the US
# this should be missing
pc_survey %>%
  filter(
    Status == "Survey Test", PreScreen_Q1 != "Yes",
    StartDate >= lubridate::ymd("2023-11-06")
  ) %>%
  mutate(Q1 = as.character(Q1)) %>%
  distinct(Q1) %>%
  is.na() %>%
  stopifnot()
```

# Examine the test data

* Check if `rnum` and `pi_i` values are working properly

```{r}
pc_survey_test <- pc_survey %>%
  filter(Status == "Survey Test") %>%
  # create 'profile' variable
  mutate(profile = case_when(
    rnum <= pi1 ~ 1,
    rnum > pi1 & rnum <= pi2 ~ 2,
    rnum > pi2 & rnum <= pi3 ~ 3,
    rnum > pi3 & rnum <= pi4 ~ 4,
    rnum > pi4 & rnum <= pi5 ~ 5,
    rnum > pi5 & rnum <= pi6 ~ 6,
    rnum > pi6 & rnum <= pi7 ~ 7,
    rnum > pi7 ~ 8
  ))


# check distribution
# this should be evenly distributed
# each should be about 1/8 of the data
nrow(pc_survey_test) / 8

pc_survey_test %>%
  ggplot(aes(profile)) +
  geom_histogram() +
  theme_classic()
```
```{r}
# create numeric versions of the profiles
# should be =1 if selecting the younger candidate
# this depends on whether the younger candidate
# is candidate 1 or candidate 2
pc_survey_recode <- pc_survey_test %>%
  mutate(across(str_c("Q", 1:8), ~ case_when(
    . == "Candidate 1" & rnum_age <= 0.5 ~ 1,
    . == "Candidate 2" & rnum_age > 0.5 ~ 1,
    # explicitly write out all the other conditions
    TRUE ~ 0
  ))) %>%
  mutate(total_Q = select(., str_c("Q", 1:8)) %>%
    rowSums())

pc_survey_test
```

```{r}
pc_survey_test
# check every respondent has exactly one non-missing value
# of Q1-Q8

# check profile matches non-missing value
pc_survey_test %>%
  filter(Status == "Survey Test") %>%
  select(str_c("Q", 1:8), profile)

# investigate why profile does not match positive question selction
pc_survey_recode %>%
  filter(Status == "Survey Test") %>%
  select(str_c("Q", 1:8), profile)

# check row total is not more than 1
# 0 if selected the older candidate
pc_survey_recode %>%
  select(str_c("Q", 1:8)) %>%
  rowSums() %>%
  is_weakly_less_than(1) %>%
  all() %>%
  stopifnot()
```

## Create fake data

```{r, setup fake data}
# create fake data
# batch size is the size of the batches (here = 100)
batch_size <- 100
```

```{r}
# pi is a vector of probabilities
# size is the number of observations
# C is number of arms
create_fake_data <- function(pi, profile_prob, size, C = 8) {
  # initialize empty dataframe
  df_fake <- data.frame(matrix(0, ncol = C, nrow = (size)))
  names(df_fake) <- str_c("Q", 1:C)
  df_fake["profile"] <- NA

  # randomly generate profile based on pi cdf
  for (i in 1:batch_size) {
    rnum <- runif(1)
    profile <- case_when(
      rnum < pi[1] ~ 1,
      rnum >= pi[1] & rnum < pi[2] ~ 2,
      rnum >= pi[2] & rnum < pi[3] ~ 3,
      rnum >= pi[3] & rnum < pi[4] ~ 4,
      rnum >= pi[4] & rnum < pi[5] ~ 5,
      rnum >= pi[5] & rnum < pi[6] ~ 6,
      rnum >= pi[6] & rnum < pi[7] ~ 7,
      rnum >= pi[7] ~ 8
    )

    # assign based on probability of choosing a younger profile
    df_fake[i, str_c("Q", profile)] <- rbinom(1, 1, profile_prob[profile])
    df_fake[i, "profile"] <- profile
  }
  return(df_fake)
}
```

```{r}
profile_prob <- c(0.9, 0.5, 0.3, 0.5, 0.4, 0.7, 0.5, 0.44) # probabilities of selecting younger candidate
pi <- cumsum(rep(0.125, 8)) # treatment assignment probabilities (CDF)

pc_survey_fake <- create_fake_data(pi, profile_prob, batch_size)
pc_survey_fake %>%
  head()
```

### Examine fake data

* Check distribution of profiles overll and by batch

```{r}
pc_survey_fake %>%
  ggplot(aes(profile)) +
  geom_histogram() +
  theme_classic()

pc_survey_fake %>%
  pivot_longer(-c(profile)) %>%
  group_by(profile) %>%
  summarize(mean = mean(value)) %>%
  ggplot(aes(profile, mean)) +
  geom_col() +
  theme_classic()

pc_survey_fake %>%
  # share of respondents who select
  # the younger candidate
  pivot_longer(-c(profile)) %>%
  group_by(profile) %>%
  summarize(
    mean = mean(value),
    total = sum(value)
  )
```

# Algorithms

Much of the following code comes from this [tutorial](https://mollyow.shinyapps.io/adaptive)

## Normal Experiment

* The treatment assignment probabilities do not change

```{r, global params}
# define with comments
num_profiles <- 8 # number of profiles
num_sim <- 1000 # number of simulations for Monte Carlo simulation
eps <- 0.1 # for epsilon greedy alg

N <- 4500 # total number of observations
num_batches <- N / batch_size # total number of batches
pi_init <- cumsum(rep(0.125, 8)) # initial pi cdf
```

```{r}
# init parameters
num_outcome1 <- integer(num_profiles) # vector for each arm
num_outcome0 <- integer(num_profiles)
pi <- lst(pi_init) # init uniform treatment assignment

df_list <- tibble()
for (n in 1:T) { # for T runs (in our case 45)
  # subset data into 100 obs and assign treatment randomly
  df_subset <- assign_treatment(pi[[n]], profile_prob, batch_size, C = c)
  df_list <- bind_rows(df_list, df_subset)
  stopifnot(nrow(df_subset) == batch_size)
  max_random <- 0
  profile_val <- 1

  # iterate through all context arms
  for (i in 1:c) {
    # draw from prior distribution for theta
    # for each arm
    # alpha, beta parameters depend
    # on the number of prior rewards
    random_theta <- rbeta(
      n = 1,
      shape1 = numbers_of_rewards_1[i] + 1,
      shape2 = numbers_of_rewards_0[i] + 1
    )

    if (random_theta > max_random) {
      # if arm is better than all the previous ones
      # this arm is selected, choose theta
      max_random <- random_theta
      profile_val <- i
    }
  }

  # after selecting the argmax, add to selected profile
  profiles_selected <- append(profiles_selected, profile_val)

  # checking with data (what is *observed*)
  # update based on reward
  obs_outcome <- df_subset %>%
    filter(profile == profile_val) %>%
    magrittr::extract2(str_c("Q", profile_val))

  reward0 <- sum(obs_outcome == 0)
  reward1 <- sum(obs_outcome == 1)
  numbers_of_rewards_0[profile_val] <- numbers_of_rewards_0[profile_val] + reward0
  numbers_of_rewards_1[profile_val] <- numbers_of_rewards_1[profile_val] + reward1

  total_reward <- total_reward + reward1

  # need to generate new pi
  # will preserve as unfirom
  pi[n + 1] <- lst(c(0.125, c))

  # calculate per-period regret
  # expectation of best arm vs. expectation of chosen arm
  regret[n] <- max(profile_prob) - mean(profile_prob[profiles_selected])
}
```

```{r}
# store impt values for comparison
pi_exp <- pi
profiles_selected_exp <- profiles_selected
regret_exp <- regret
```

### Normal experiment: plotting results

```{r, message = F, warning=F}
plot_selected_profiles(profiles_selected_exp)

plot_observed_profiles(df_list)

plot_dist(numbers_of_rewards_1, numbers_of_rewards_0, c)

plot_regret(regret_exp, name = "Experiment (no probability assignment updating)")
```

## Greedy Algorithm

* use the expected values of the Beta distribution

```{r}
# init parameters
profiles_selected <- integer(0)
numbers_of_rewards_1 <- integer(c) # vector for each arm
numbers_of_rewards_0 <- integer(c)
total_reward <- 0
pi <- lst(pi_init) # init uniform treatment assignment
regret <- lst()

for (n in 1:T) { # for T runs (in our case 45)
  # subset data into 100 obs and assign treatment randomly
  df_subset <- assign_treatment(pi[[n]], profile_prob, batch_size, C = c)
  stopifnot(nrow(df_subset) == batch_size)
  max_random <- 0
  profile_val <- 1

  # iterate through all context arms
  for (i in 1:c) {
    # draw from prior distribution for theta
    # for each arm
    # alpha, beta parameters depend
    # on the number of prior rewards
    alpha <- numbers_of_rewards_1[i] + 1
    beta <- numbers_of_rewards_0[i] + 1
    exp_theta <- alpha / (alpha + beta)

    if (exp_theta > max_random) {
      # if arm is better than all the previous ones
      # this arm is selected, choose theta
      max_random <- random_theta
      profile_val <- i
    }
  }

  # after selecting the argmax, add to selected profile
  profiles_selected <- append(profiles_selected, profile_val)

  # checking with data (what is *observed*)
  # update based on reward
  obs_outcome <- df_subset %>%
    filter(profile == profile_val) %>%
    magrittr::extract2(str_c("Q", profile_val))

  reward0 <- sum(obs_outcome == 0)
  reward1 <- sum(obs_outcome == 1)
  numbers_of_rewards_0[profile_val] <- numbers_of_rewards_0[profile_val] + reward0
  numbers_of_rewards_1[profile_val] <- numbers_of_rewards_1[profile_val] + reward1

  total_reward <- total_reward + reward1

  # need to generate new pi
  # this is the probability each arm is the best
  # we can do this by simulating R random samples and calculating the probability
  # that each arm is optimal

  # Approximate Thompson Sampling probabilities (i.e. prob. that each arm is maximal)
  # (this is from Offer-Westort et al.)
  # 1) Sample from each of the posteriors R times
  draws <- replicate(R, rbeta(c, numbers_of_rewards_1, numbers_of_rewards_0))
  # 2) Check how many times each arm was maximal
  argmax <- apply(draws, 2, which.max)
  # 3) Tally up the probabilities
  pi[n + 1] <- lst(table(cut(argmax, 0:c)) / R)

  # calculate per-period regret
  # expectation of best arm vs. expectation of chosen arm
  regret[n] <- max(profile_prob) - mean(profile_prob[profiles_selected])
}
```

```{r}
# store impt values for comparison
pi_greedy <- pi
profiles_selected_greedy <- profiles_selected
regret_greedy <- regret
```

### Greedy Bernoulli: plotting results

```{r, warning=F}
plot_selected_profiles(profiles_selected)

plot_observed_profiles(df_list)

plot_dist(numbers_of_rewards_1, numbers_of_rewards_0, c)

plot_regret(regret_greedy, name = "Greedy TS")
```

## Thompson Sampling

```{r}
# init parameters
profiles_selected <- integer(0)
numbers_of_rewards_1 <- integer(c) # num_outcome1
numbers_of_rewards_0 <- integer(c) # num_outcome0
total_reward <- 0 # delete if not being used
pi <- lst(pi_init) # init uniform treatment assignment
regret <- lst()

df_list <- tibble()
for (n in 1:T) { # for T runs (in our case 45)
  # subset data into 100 obs and assign treatment randomly
  df_subset <- assign_treatment(pi[[n]], profile_prob, batch_size, C = c)
  df_list <- bind_rows(df_list, df_subset)
  stopifnot(nrow(df_subset) == batch_size)


  # second function to generate pi cdf
  # put the rest in a function
  # make sure to update numbers_of_rewards_1, numbers_of_rewards_0

  # need to generate new pi
  # this is the probability each arm is the best
  # we can do this by simulating R random samples and calculating the probability
  # that each arm is optimal

  # Approximate Thompson Sampling probabilities (i.e. prob. that each arm is maximal)
  # (this is from Offer-Westort et al.)
  # 1) Sample from each of the posteriors R times
  draws <- replicate(R, rbeta(c, numbers_of_rewards_1, numbers_of_rewards_0))
  # 2) Check how many times each arm was maximal
  argmax <- apply(draws, 2, which.max)
  # 3) Tally up the probabilities
  pi[n + 1] <- lst(table(cut(argmax, 0:c)) / R)

  # calculate per-period regret
  # expectation of best arm vs. expectation of chosen arm
  regret[n] <- max(profile_prob) - mean(profile_prob[profiles_selected])
}
```

```{r}
# store impt values for comparison
pi_ts <- pi
regret_ts <- regret
```

### TS: plotting results

```{r, message = F, warning=F}
plot_selected_profiles(profiles_selected_ts)

plot_observed_profiles(df_list)

plot_dist(numbers_of_rewards_1, numbers_of_rewards_0, c)

plot_regret(regret_ts)
```

## Epsilon Greedy

* From Offer-Westort et al. (2020): in the Epsilon Greedy algorithm, we identify which arm has the highest mean outcome. We assign treatment to that arm in (1âˆ’\epsilon)  portion of assignments, and assign treatment uniformly at random in the remaining \epsilon assignments.
* Note: can also allow epsilon to decay over time

```{r}
# init parameters
profiles_selected <- integer(0)
numbers_of_rewards_1 <- integer(c) # vector for each arm
numbers_of_rewards_0 <- integer(c)
total_reward <- 0
pi <- lst(pi_init) # init uniform treatment assignment
regret <- lst()

for (n in 1:T) { # for T runs (in our case 45)
  # subset data into 100 obs and assign treatment randomly
  df_subset <- assign_treatment(pi[[n]], profile_prob, batch_size, C = c)
  df_list <- bind_rows(df_list, df_subset)
  stopifnot(nrow(df_subset) == batch_size)
  max_random <- 0
  profile_val <- 1

  # identify which arm has highest mean outcome
  sorted_df <- df_subset %>%
    pivot_longer(-profile, names_to = "arm", values_to = "reward") %>%
    group_by(profile) %>%
    summarize(mean_reward = mean(reward)) %>%
    arrange(desc(mean_reward))

  # highest arm is the first one
  profile_val <- sorted_df[1, ][["profile"]]

  # after selecting the argmax, add to selected profile
  profiles_selected <- append(profiles_selected, profile_val)

  # checking with data (what is *observed*)
  # update based on reward
  obs_outcome <- df_subset %>%
    filter(profile == profile_val) %>%
    magrittr::extract2(str_c("Q", profile_val))

  reward0 <- sum(obs_outcome == 0)
  reward1 <- sum(obs_outcome == 1)
  numbers_of_rewards_0[profile_val] <- numbers_of_rewards_0[profile_val] + reward0
  numbers_of_rewards_1[profile_val] <- numbers_of_rewards_1[profile_val] + reward1

  total_reward <- total_reward + reward1

  # need to generate new pi
  pi_n <- rep(eps, c)
  pi_n[profile_val] <- 1 - eps
  pi[n + 1] <- lst(pi_n)

  # calculate per-period regret
  # expectation of best arm vs. expectation of chosen arm
  regret[n] <- max(profile_prob) - mean(profile_prob[profiles_selected])
}
```

```{r}
# store impt values for comparison
pi_eg <- pi
profiles_selected_eg <- profiles_selected
regret_eg <- regret
```

### Epsilon-Greedy: plotting results

```{r, message = F, warning=F}
plot_selected_profiles(profiles_selected_eg, name = "Epsilon Greedy")

plot_observed_profiles(df_list)

plot_dist(numbers_of_rewards_1, numbers_of_rewards_0, c)

plot_regret(regret_eg, name = "Epsilon Greedy Regret")
```

## UCB (Upper Confidence Bound)

* observe sample means under each of the arms, and then we compute uncertainty bounds around each of those estimates (note that these are not the same as confidence intervals under a normal approximation)
* goal is to reduce uncertainty

```{r}
# init parameters
profiles_selected <- integer(0)
numbers_of_rewards_1 <- integer(c) # vector for each arm
numbers_of_rewards_0 <- integer(c)
total_reward <- 0
pi <- lst(pi_init) # init uniform treatment assignment
regret <- lst()

for (n in 1:T) { # for T runs (in our case 45)
  # subset data into 100 obs and assign treatment randomly
  df_subset <- assign_treatment(pi[[n]], profile_prob, batch_size, C = c)
  df_list <- bind_rows(df_list, df_subset)
  stopifnot(nrow(df_subset) == batch_size)
  max_random <- 0
  profile_val <- 1

  # identify ucb arm
  sorted_df <- df_subset %>%
    pivot_longer(-profile, names_to = "arm", values_to = "success") %>%
    group_by(profile) %>%
    summarize(
      successes = sum(success),
      trials = n()
    ) %>%
    ungroup() %>%
    mutate(ucb_arm = successes / trials + sqrt(1 * log(sum(trials)) / trials)) %>%
    arrange(desc(ucb_arm))

  # update selected arm
  profile_val <- sorted_df[1, ][["profile"]]

  # after selecting the argmax, add to selected profile
  profiles_selected <- append(profiles_selected, profile_val)

  # checking with data (what is *observed*)
  # update based on reward
  obs_outcome <- df_subset %>%
    filter(profile == profile_val) %>%
    magrittr::extract2(str_c("Q", profile_val))

  reward0 <- sum(obs_outcome == 0)
  reward1 <- sum(obs_outcome == 1)
  numbers_of_rewards_0[profile_val] <- numbers_of_rewards_0[profile_val] + reward0
  numbers_of_rewards_1[profile_val] <- numbers_of_rewards_1[profile_val] + reward1

  total_reward <- total_reward + reward1

  # need to generate new pi
  pi_n <- rep(0, c)
  pi_n[profile_val] <- 1
  pi[n + 1] <- lst(pi_n)

  # calculate per-period regret
  # expectation of best arm vs. expectation of chosen arm
  regret[n] <- max(profile_prob) - mean(profile_prob[profiles_selected])
}
```

```{r}
# store impt values for comparison
pi_ucb <- pi
profiles_selected_ucb <- profiles_selected
regret_ucb <- regret
```

### UCB: plotting results

```{r, message = F, warning=F}
plot_selected_profiles(profiles_selected_ucb, name = "UCB")

plot_observed_profiles(df_list)

plot_dist(numbers_of_rewards_1, numbers_of_rewards_0, c)

plot_regret(regret_ucb, name = "UCB Regret")
```

## Compare treatment assignments over time

```{r}
tibble(
  "pi" = map(2:T, ~ pi_greedy[[.]]),
  "arm" = map(2:T, ~ c(1:8))
) %>%
  mutate(id = row_number()) %>%
  unnest(cols = c(pi, arm)) %>%
  ggplot(aes(id, pi)) +
  geom_line() +
  facet_wrap(~arm) +
  theme_classic() +
  labs(
    title = "Greedy Bernoulli Treatment Assignments over time",
    x = "Time",
    y = "Probability of being the best arm"
  ) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2))

tibble(
  "pi" = map(2:T, ~ pi_ts[[.]]),
  "arm" = map(2:T, ~ c(1:8))
) %>%
  mutate(id = row_number()) %>%
  unnest(cols = c(pi, arm)) %>%
  ggplot(aes(id, pi)) +
  geom_line() +
  facet_wrap(~arm) +
  theme_classic() +
  labs(
    title = "TS Treatment Assignments over time",
    x = "Time",
    y = "Probability of being the best arm"
  ) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2))

tibble(
  "pi" = map(2:T, ~ pi_eg[[.]]),
  "arm" = map(2:T, ~ c(1:8))
) %>%
  mutate(id = row_number()) %>%
  unnest(cols = c(pi, arm)) %>%
  ggplot(aes(id, pi)) +
  geom_line() +
  facet_wrap(~arm) +
  theme_classic() +
  labs(
    title = "Epsilon-Greedy Treatment Assignments over time",
    x = "Time",
    y = "Probability of being the best arm"
  ) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2))
```

```{r}
# put all types of regret onto one plot
tibble(
  "t" = c(1:length(regret_exp)),
  "Regret (no update)" = unlist(regret_exp),
  "Greedy TS" = unlist(regret_greedy),
  "Thompson Sampling Regret" = unlist(regret_ts),
  "Epsilon Greedy Regret" = unlist(regret_eg),
  "UCB Regret" = unlist(regret_ucb)
) %>%
  pivot_longer(-t, names_to = "Algorithm", values_to = "Regret") %>%
  ggplot(aes(t, Regret, color = Algorithm)) +
  geom_line() +
  theme_classic()
```


